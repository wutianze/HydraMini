\section{Related Work}
In this section, we summarize the related work and several other similar platforms. Compared to above products, our platform costs less and provides FPGA support, also it's easy for users to custom their own components based on it. The basic of our tool kit is simple and affordable, but it has great potential. \textbf{Table 1} shows the comparison between these platforms.

\textbf{Research Platform for Autonomous Devices.} Wang et al. presented HydraOne\cite{wang2019hydraone}, HydraOne is an indoor robot-based platform, it has sufficient resources and components to conduct related experiments. It has three key characteristics: design modularization, resource extensibility and openness, as well as function isolation, which allows users to conduct various research and education experiments. Wei et al. presented the CMU autonomous driving research platform which is based on a Cadillac SRX\cite{wei2013towards}. This work focuses on vehicle engineering problems, including the actuation, power, and sensor systems on the vehicle. Matthew Oâ€™Kelly et al present F1/10\cite{o2019f1}: an open-source, affordable, and high-performance 1/10 scale autonomous vehicle testbed. The F1/10 testbed carries a full suite of sensors, perception, planning, control, and networking software stacks that are similar to full scale solutions. 

\textbf{Hardware Acceleration Technology used in AI.} GPU\cite{nurvitadhi2016accelerating} is now widely used by researchers to accelerate the training and inference process of AI. The training library is cuDNN\cite{cudnn} while the inference library is TensorRT\cite{tensorrt}. cuDNN is a GPU-accelerated library of primitives for deep neural networks. It provides highly tuned implementations of routines arising frequently in DNN applications. TensorRT focuses specifically on running an already trained network quickly and efficiently on a GPU for the purpose of generating a result; also known as inferencing. The tensor processing unit was announced in May 2016 at Google I/O, when the company said that the TPU had already been used inside their data centers for over a year\cite{techradar}. The chip has been specifically designed for Google's TensorFlow framework, a symbolic math library which is used for machine learning applications such as neural networks.\cite{tensorprocessingunit} The Xilinx Deep Learning Processor Unit (DPU)\cite{dnndk} is a programmable engine optimized for convolutional neural networks. The unit includes a high performance scheduler module, a hybrid computing array module, an instruction fetch unit module, and a global memory pool module. The DPU uses a specialized instruction set, which allows for the efficient implementation of many convolutional neural network. With our platform, you are able to easily try DPU or FPGA to help your algorithms run better.