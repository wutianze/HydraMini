# What you will learn
- How to use dnndk to do quantization to the model.
- How to use dnndk to compile the model(make DPU understandable .elf files).
- How to use Library libn2cube to use DPU.

## Transfer keras model to tensorflow model
Transfer the .h5 model to .pb model using `python keras_to_tensorflow.py --input_model="./model.h5" --output_model="./model.pb"` and then print the model's info using `decent_q inspect --input_frozen_graph=./model.pb`.
`keras_to_tensorflow.py` transfers .h5 model to .pb model.
![Keras to Tensorflow](./imgs/keras_to_tensorflow.png)
`decent_q inspect` command below prints the CONV_INPUT(input_nodes) name and CONV_OUTPUT(output_nodes) name of the model.
![decent_q inspect](./imgs/inspect_pb.png)

## Quantization and Compilation
### Steps
1. Edit `quant.sh`, change the `--input_nodes conv2d_1_input --input_shapes ?,80,160,3 --output_nodes dense_3/Softmax` according to the info you get from `decent_q inspect` and the model input image size.  
2. Edit `graph_input_fn.py`, change the `CONV_INPUT` to `--input_nodes` in `quant.sh` and set the read path of glob to the copied images directory(`images/`). The `CUT_SIZE` should be same as the parameter for `train.py`. 
3. Edit `compile.sh`, make a name for your model by changing `--net_name`. ***For dnndk v3.1, you need to add `--dcf ./pynqz2.dcf`, the .dcf file can be generated by `dlet -f pynqzx_dpu.hwh`. For more information about dnndk3.1, please read related documents.***
4. Run `./quant.sh && compile.sh`, it will do quantization to the trained model and build .elf files which can be used by DPU. You can see the info printed in your screen if nothing goes wrong. You must remember the info printed and they will be used later in Pynq-Part. 
![quant](./imgs/quant.png)
![compile](./imgs/compile.png)
5. Copy the .elf files in `compile/` to the `Car/model/` and then move the `Car` directory to your Pynq-Z2 board.

## Run .elf in DPU
### DPU c++ library libn2cube usage: 
- Library libn2cube is the DNNDK core library. It implements the functionality of DPU loader, and encapsulates the system calls to invoke the DPU driver for DPU Task scheduling, monitoring, profiling, and resources management. 
- Simple guide of libn2cube
  1. The information you need to know after the generation of .elf using the host part of DNNDK-v3.0(If you don't know what it is, please refer to the Host part guide of this project) is:
     -  kernel name: KERNEL_CONV
     -  input node(s): CONV_INPUT_NODE
     -  output node(s): CONV_OUTPUT_NODE  
  2. Each DPU kernel supported by the DPU has a corresponding ELF object file with a name that is the same as the kernel name prefixed by dpu_ with extension .elf. So before compiling your code, you should put the .elf files into model directory and add their names to MODEL variable in Makefile.  
  3. The sample flow of using DPU kernel:  
    ```c++
    int main(void) {
    /* DPU Kernels/Tasks for running ResNet-50 */
    DPUKernel* kernelConv;
    DPUTask* taskConv
    /* Attach to DPU driver and prepare for running */
    dpuOpen()
    /* Create DPU Kernels for CONV Nodes */
    kernelConv = dpuLoadKernel(KERNEL_CONV)
    /* Create DPU Tasks for CONV Nodes*/
    taskConv = dpuCreateTask(kernelConv, 0)
    /* Run CONV Kernel*/
    Mat image = imread(baseImagePath + imageName);
    dpuSetInputImage2(taskConv, CONV_INPUT_NODE, image);
    dpuRunTask(taskConv)
    /* Get inference result */
    float scale = dpuGetOutputTensorScale(taskConv, CONV_OUTPUT_NODE);
    /* modelRes is the start address of the model's output */
    int8_t* modelRes = dpuGetTensorAddress(dpuGetOutputTensor(taskConv, CONV_OUTPUT_NODE));
    /* the true value should * scale */
    float result1 = modelRes[0] * scale;

    /* Destroy DPU Tasks & release resources */
    dpuDestroyTask(taskConv)
    /* Destroy DPU Kernel & release resources */
    dpuDestroyKernel(kernelConv)
    /* Detach DPU driver & release resources */
    dpuClose();
    return 0
    ```
  4. The `main()` operations include:
     - Call `dpuOpen()` to open the DPU device.
     - Call `dpuLoadKernel()` to load the DPU kernel reset50_0.
     - Call `dpuCreateTask()` to create task for each DPU kernel.
     - Call `dpuDestroyKernel()` and `dpuDestroyTask()` to destroy DPU kernel and task.
     - Call `dpuClose()` to close the DPU device.
  5. Run CONV Kernel include:
     - Get the image for processing and set it as input
     - Run the task using `dpuRunTask()`
     - Get the output of the kernel and Run Softmax in it if needed
     - If your model is regression model, you can just use the result without running softmax like  
    ```c++
    float* res = float*(smRes.data()); float final_res = res[0] + res[1]
    ``` 
- In the car:
```c++
int channel = kinds.size();//kinds={"steer","throttle"}, os channel=2
vector<float> smRes(channel);

Mat tmpImage;
while (1)
{
   if(!takenImages.try_pop(tmpImage))continue;// get input

   _T(setInputImage(task, CONV_INPUT_NODE, tmpImage));// input

   _T(dpuRunTask(task));
   float scale = dpuGetOutputTensorScale(task, CONV_OUTPUT_NODE);
   int8_t* modelRes = dpuGetTensorAddress(dpuGetOutputTensor(task, CONV_OUTPUT_NODE));// get output address
   float steer = modelRes[0] * scale * 2 - 1.0;// output 1: steer
   float throttle = modelRes[1] * scale * 2 - 1.0;// output 2: throttle

	cout<<"output steer:"<<steer<<" throttle:"<<throttle<<endl;  
}

```

# References
[libn2cube API](https://www.xilinx.com/support/documentation/sw_manuals/ai_inference/v1_5/ug1327-dnndk-user-guide.pdf)
